SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/mnt/yarn/usercache/hadoop/filecache/10/__spark_libs__819149632355513758.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
19/01/25 15:22:20 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 10001@ip-172-31-15-145
19/01/25 15:22:20 INFO SignalUtils: Registered signal handler for TERM
19/01/25 15:22:20 INFO SignalUtils: Registered signal handler for HUP
19/01/25 15:22:20 INFO SignalUtils: Registered signal handler for INT
19/01/25 15:22:21 INFO SecurityManager: Changing view acls to: yarn,hadoop
19/01/25 15:22:21 INFO SecurityManager: Changing modify acls to: yarn,hadoop
19/01/25 15:22:21 INFO SecurityManager: Changing view acls groups to: 
19/01/25 15:22:21 INFO SecurityManager: Changing modify acls groups to: 
19/01/25 15:22:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, hadoop); groups with view permissions: Set(); users  with modify permissions: Set(yarn, hadoop); groups with modify permissions: Set()
19/01/25 15:22:21 INFO TransportClientFactory: Successfully created connection to ip-172-31-12-227.ec2.internal/172.31.12.227:38289 after 131 ms (0 ms spent in bootstraps)
19/01/25 15:22:22 INFO SecurityManager: Changing view acls to: yarn,hadoop
19/01/25 15:22:22 INFO SecurityManager: Changing modify acls to: yarn,hadoop
19/01/25 15:22:22 INFO SecurityManager: Changing view acls groups to: 
19/01/25 15:22:22 INFO SecurityManager: Changing modify acls groups to: 
19/01/25 15:22:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, hadoop); groups with view permissions: Set(); users  with modify permissions: Set(yarn, hadoop); groups with modify permissions: Set()
19/01/25 15:22:22 INFO TransportClientFactory: Successfully created connection to ip-172-31-12-227.ec2.internal/172.31.12.227:38289 after 17 ms (0 ms spent in bootstraps)
19/01/25 15:22:22 INFO DiskBlockManager: Created local directory at /mnt/yarn/usercache/hadoop/appcache/application_1548428999420_0001/blockmgr-8133a1e3-b9e5-4249-b3ac-2f9d9c099ccc
19/01/25 15:22:22 INFO MemoryStore: MemoryStore started with capacity 2.8 GB
19/01/25 15:22:22 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@ip-172-31-12-227.ec2.internal:38289
19/01/25 15:22:22 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
19/01/25 15:22:22 INFO Executor: Starting executor ID 2 on host ip-172-31-15-145.ec2.internal
19/01/25 15:22:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45987.
19/01/25 15:22:23 INFO NettyBlockTransferService: Server created on ip-172-31-15-145.ec2.internal:45987
19/01/25 15:22:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/01/25 15:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(2, ip-172-31-15-145.ec2.internal, 45987, None)
19/01/25 15:22:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(2, ip-172-31-15-145.ec2.internal, 45987, None)
19/01/25 15:22:23 INFO BlockManager: external shuffle service port = 7337
19/01/25 15:22:23 INFO BlockManager: Registering executor with local external shuffle service.
19/01/25 15:22:23 INFO TransportClientFactory: Successfully created connection to ip-172-31-15-145.ec2.internal/172.31.15.145:7337 after 21 ms (0 ms spent in bootstraps)
19/01/25 15:22:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(2, ip-172-31-15-145.ec2.internal, 45987, None)
19/01/25 15:22:23 INFO CoarseGrainedExecutorBackend: Got assigned task 12
19/01/25 15:22:23 INFO CoarseGrainedExecutorBackend: Got assigned task 13
19/01/25 15:22:23 INFO CoarseGrainedExecutorBackend: Got assigned task 14
19/01/25 15:22:23 INFO CoarseGrainedExecutorBackend: Got assigned task 15
19/01/25 15:22:23 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)
19/01/25 15:22:23 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)
19/01/25 15:22:23 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)
19/01/25 15:22:23 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)
19/01/25 15:22:23 INFO Executor: Fetching spark://ip-172-31-12-227.ec2.internal:38289/jars/spark-demo.jar with timestamp 1548429718049
19/01/25 15:22:23 INFO TransportClientFactory: Successfully created connection to ip-172-31-12-227.ec2.internal/172.31.12.227:38289 after 20 ms (0 ms spent in bootstraps)
19/01/25 15:22:23 INFO Utils: Fetching spark://ip-172-31-12-227.ec2.internal:38289/jars/spark-demo.jar to /mnt/yarn/usercache/hadoop/appcache/application_1548428999420_0001/spark-bc67c409-2ae8-44e9-ab04-0acffbe98d12/fetchFileTemp7836799403725558214.tmp
19/01/25 15:22:23 INFO Utils: Copying /mnt/yarn/usercache/hadoop/appcache/application_1548428999420_0001/spark-bc67c409-2ae8-44e9-ab04-0acffbe98d12/9769077931548429718049_cache to /mnt/yarn/usercache/hadoop/appcache/application_1548428999420_0001/container_1548428999420_0001_01_000003/./spark-demo.jar
19/01/25 15:22:23 INFO Executor: Adding file:/mnt/yarn/usercache/hadoop/appcache/application_1548428999420_0001/container_1548428999420_0001_01_000003/./spark-demo.jar to class loader
19/01/25 15:22:24 INFO TorrentBroadcast: Started reading broadcast variable 1
19/01/25 15:22:24 INFO TransportClientFactory: Successfully created connection to ip-172-31-9-171.ec2.internal/172.31.9.171:46519 after 4 ms (0 ms spent in bootstraps)
19/01/25 15:22:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 2.8 GB)
19/01/25 15:22:24 INFO TorrentBroadcast: Reading broadcast variable 1 took 286 ms
19/01/25 15:22:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.8 KB, free 2.8 GB)
19/01/25 15:22:24 INFO HadoopRDD: Input split: s3://mr-input-hw1/input/edges.csv:1006632960+67108864
19/01/25 15:22:24 INFO HadoopRDD: Input split: s3://mr-input-hw1/input/edges.csv:872415232+67108864
19/01/25 15:22:24 INFO HadoopRDD: Input split: s3://mr-input-hw1/input/edges.csv:939524096+67108864
19/01/25 15:22:24 INFO HadoopRDD: Input split: s3://mr-input-hw1/input/edges.csv:805306368+67108864
19/01/25 15:22:24 INFO TorrentBroadcast: Started reading broadcast variable 0
19/01/25 15:22:24 INFO TransportClientFactory: Successfully created connection to ip-172-31-15-145.ec2.internal/172.31.15.145:45999 after 37 ms (0 ms spent in bootstraps)
19/01/25 15:22:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.7 KB, free 2.8 GB)
19/01/25 15:22:24 INFO TorrentBroadcast: Reading broadcast variable 0 took 199 ms
19/01/25 15:22:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 317.3 KB, free 2.8 GB)
19/01/25 15:22:26 INFO GPLNativeCodeLoader: Loaded native gpl library
19/01/25 15:22:26 INFO LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev bab859f34a291cb7b3f4e724b59e1b48af69016b]
19/01/25 15:22:30 INFO S3NativeFileSystem: Opening 's3://mr-input-hw1/input/edges.csv' for reading
19/01/25 15:22:30 INFO S3NativeFileSystem: Opening 's3://mr-input-hw1/input/edges.csv' for reading
19/01/25 15:22:30 INFO S3NativeFileSystem: Opening 's3://mr-input-hw1/input/edges.csv' for reading
19/01/25 15:22:30 INFO S3NativeFileSystem: Opening 's3://mr-input-hw1/input/edges.csv' for reading
19/01/25 15:22:57 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1227 bytes result sent to driver
19/01/25 15:22:57 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1184 bytes result sent to driver
19/01/25 15:22:58 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1184 bytes result sent to driver
19/01/25 15:22:58 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1184 bytes result sent to driver
19/01/25 15:23:04 INFO CoarseGrainedExecutorBackend: Got assigned task 21
19/01/25 15:23:04 INFO CoarseGrainedExecutorBackend: Got assigned task 23
19/01/25 15:23:04 INFO CoarseGrainedExecutorBackend: Got assigned task 25
19/01/25 15:23:04 INFO CoarseGrainedExecutorBackend: Got assigned task 27
19/01/25 15:23:04 INFO Executor: Running task 5.0 in stage 1.0 (TID 25)
19/01/25 15:23:04 INFO Executor: Running task 1.0 in stage 1.0 (TID 21)
19/01/25 15:23:04 INFO Executor: Running task 3.0 in stage 1.0 (TID 23)
19/01/25 15:23:04 INFO Executor: Running task 7.0 in stage 1.0 (TID 27)
19/01/25 15:23:04 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
19/01/25 15:23:04 INFO TorrentBroadcast: Started reading broadcast variable 2
19/01/25 15:23:04 INFO TransportClientFactory: Successfully created connection to ip-172-31-12-227.ec2.internal/172.31.12.227:42907 after 12 ms (0 ms spent in bootstraps)
19/01/25 15:23:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 26.9 KB, free 2.8 GB)
19/01/25 15:23:04 INFO TorrentBroadcast: Reading broadcast variable 2 took 50 ms
19/01/25 15:23:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 72.3 KB, free 2.8 GB)
19/01/25 15:23:05 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
19/01/25 15:23:05 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@ip-172-31-12-227.ec2.internal:38289)
19/01/25 15:23:05 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
19/01/25 15:23:05 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
19/01/25 15:23:05 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
19/01/25 15:23:05 INFO MapOutputTrackerWorker: Got the output locations
19/01/25 15:23:05 INFO ShuffleBlockFetcherIterator: Getting 20 non-empty blocks including 4 local blocks and 16 remote blocks
19/01/25 15:23:05 INFO ShuffleBlockFetcherIterator: Getting 20 non-empty blocks including 4 local blocks and 16 remote blocks
19/01/25 15:23:05 INFO ShuffleBlockFetcherIterator: Getting 20 non-empty blocks including 4 local blocks and 16 remote blocks
19/01/25 15:23:05 INFO ShuffleBlockFetcherIterator: Getting 20 non-empty blocks including 4 local blocks and 16 remote blocks
19/01/25 15:23:05 INFO TransportClientFactory: Successfully created connection to ip-172-31-15-145.ec2.internal/172.31.15.145:7337 after 49 ms (0 ms spent in bootstraps)
19/01/25 15:23:05 INFO TransportClientFactory: Successfully created connection to ip-172-31-10-234.ec2.internal/172.31.10.234:7337 after 22 ms (0 ms spent in bootstraps)
19/01/25 15:23:05 INFO TransportClientFactory: Successfully created connection to ip-172-31-9-171.ec2.internal/172.31.9.171:7337 after 131 ms (0 ms spent in bootstraps)
19/01/25 15:23:05 INFO TransportClientFactory: Successfully created connection to ip-172-31-8-9.ec2.internal/172.31.8.9:7337 after 29 ms (0 ms spent in bootstraps)
19/01/25 15:23:05 INFO ShuffleBlockFetcherIterator: Started 4 remote fetches in 221 ms
19/01/25 15:23:05 INFO ShuffleBlockFetcherIterator: Started 4 remote fetches in 214 ms
19/01/25 15:23:05 INFO ShuffleBlockFetcherIterator: Started 4 remote fetches in 230 ms
19/01/25 15:23:05 INFO ShuffleBlockFetcherIterator: Started 4 remote fetches in 224 ms
19/01/25 15:23:08 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.DirectFileOutputCommitter
19/01/25 15:23:08 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.DirectFileOutputCommitter
19/01/25 15:23:08 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.DirectFileOutputCommitter
19/01/25 15:23:08 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.DirectFileOutputCommitter
19/01/25 15:23:10 INFO MultipartUploadOutputStream: close closed:false s3://mr-input-hw1/output/part-00003
19/01/25 15:23:10 INFO MultipartUploadOutputStream: close closed:false s3://mr-input-hw1/output/part-00005
19/01/25 15:23:10 INFO MultipartUploadOutputStream: close closed:false s3://mr-input-hw1/output/part-00007
19/01/25 15:23:10 INFO MultipartUploadOutputStream: close closed:false s3://mr-input-hw1/output/part-00001
19/01/25 15:23:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20190125152213_0005_m_000001_0
19/01/25 15:23:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20190125152213_0005_m_000003_0
19/01/25 15:23:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20190125152213_0005_m_000005_0
19/01/25 15:23:11 INFO Executor: Finished task 3.0 in stage 1.0 (TID 23). 1502 bytes result sent to driver
19/01/25 15:23:11 INFO Executor: Finished task 5.0 in stage 1.0 (TID 25). 1502 bytes result sent to driver
19/01/25 15:23:11 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20190125152213_0005_m_000007_0
19/01/25 15:23:11 INFO Executor: Finished task 7.0 in stage 1.0 (TID 27). 1502 bytes result sent to driver
19/01/25 15:23:11 INFO Executor: Finished task 1.0 in stage 1.0 (TID 21). 1545 bytes result sent to driver
19/01/25 15:23:12 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
19/01/25 15:23:12 INFO MemoryStore: MemoryStore cleared
19/01/25 15:23:12 INFO BlockManager: BlockManager stopped
19/01/25 15:23:12 INFO ShutdownHookManager: Shutdown hook called
19/01/25 15:23:12 INFO ShutdownHookManager: Deleting directory /mnt/yarn/usercache/hadoop/appcache/application_1548428999420_0001/spark-bc67c409-2ae8-44e9-ab04-0acffbe98d12
